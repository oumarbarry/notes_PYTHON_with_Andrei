Scraping Data with Python:
	Web Scraping:
		it simply means programmatically using Python or another language, to grab data from a website
		any 'public' website can be scrapped
		
		essentially when we talk about web scraping, we mean downloading a web page,
			grabbing the data from that web page, and cleaning it for making that data useful for us
				and then use that data
		
		entropywebscraping.com: Big List of Web Scraping Usage - Application of Web Scraping to Buisness
		chrome web scraper plugin
		
	Web Scraping and APIs:
		there's a way for a website to tell us what can or can not be scrape
			we simply add /robots.txt at the end of his url
				www.airbnb.ca/robots.txt	
		so there is ethical and unethical ways of scraping websites
			ethical if you follow the robots.txt rules of a website
			
		remember the easiest way to get data from a website is usually using an API like with:
			developer.github.com/v3/: github api - which required to have an validate account
			swapi.co or jsonplaceholder.typicode.com - which are free to access	
		but sometimes a website doesn't offer an API, where comes web scrapping
		
	Hacker News Project: let's scrape Hacker News / news.ycombinator.com
		
		after try to build a scraper about: 
			hacker news who is hiring ? (where company posts job opportunities for programmers each month)
			
		BeautifulSoup:
			pip install beautifulsoup4
			Beautiful Soup allows us to use the html and grab different data
				so it allows us to use that data that we've gathered to do whatever we want to it
			the requests module is actually what allows us to download initially that html
			
			example:
				import requests
				from bs4 import BeautifulSoup
				
				res = requests.get('https://news.ycombinator.com/news')
				soup = BeautifulSoup(res.text, 'html.parser')
				
				#BeautifulSoup Basics:
					#soup.body, soup.body.contents, soup.title, soup.a
					#soup.find_all('div'), soup.find('a'), soup.find(id='score_20514755')
				
				#BeautifulSoup Selectors: use the css selectors to grab html element
					#soup.select('div'),  soup.select('.score'), soup.select('#score_20514755')
				
				links = soup.select('.storylink')
				votes = soup.select('.score')
				
				#side note: with bs4 we can chain mutiple select method to grab deeper informations
					#also can do: print( votes[0].get('id') )
					
					
			scrape.py
				import requests
				from bs4 import BeautifulSoup
				from pprint import pprint #pretty print built-in module
				
				
				def list_of_page_requested(numberOfPages):
					mega_links, mega_subtext = [], []
					for i in range(numberOfPages - 1):
						if i==0:
							res = requests.get('https://news.ycombinator.com/news')
							soup = BeautifulSoup(res.text, 'html.parser')
							links = soup.select('.storylink')
							subtext = soup.select('.subtext')
							mega_links.extend(links)
							mega_subtext.extend(links)
						else:
							res = requests.get(f'https://news.ycombinator.com/news?p={i}')
							soup = BeautifulSoup(res.text, 'html.parser')
							links = soup.select('.storylink')
							subtext = soup.select('.subtext')
							mega_links.extend(links)
							mega_subtext.extend(links)
							
					return mega_links, mega_subtext
						
				def sorted_hn_by_decreasing_vote(hn_list):
					return hn_list.sort(key="lambda d: d['votes']", reverse=True) 
					#== return sorted(hn_list, key="lambda d: d['votes']", reverse='True')
			
				def create_custom_hacker_news(links, subtext):
					hn_list = []
					for index, link in enumerate(links):
						title = link.getText()
						href = link.get('href')
						vote = subtext[index].select('.score')
						if len(vote):
							points = int(vote[0].getText().replace(' points', ''))
							if points >= 100:
								hn_list.append( {'title': title, 'link': href, 'votes': points} )
					
					return sorted_hn_by_decreasing_vote(hn_list)
				
				link_list, subtext_list = list_of_page_requested(2)
				pprint(create_custom_hacker_news( link_list, subtext_list ))
			
	What to do next with Scraping:
		well first get familiar with BeautifulSoup
		but keep in mind ideally we don't even have to webscrape 
			if the target website already give us an API, 
				so in that case, first start reading about what data they give us access
				
		the next step can be the framework Scrapy:
			pip install scrapy
			mind that we mainly use things like scrappy 
				when we want to scrape massive websites, massive amounts of data
		
		and eventually we may want to store all this data in a text file, in a csv file
			but as we collect more and more data, think about databases